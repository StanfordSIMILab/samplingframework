{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FVI Clustering\n",
    "\n",
    "This notebook filters frames based on FVI scores, creates embeddings for the selected frames, and clusters the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "# Add the path to the scripts directory\n",
    "import sys\n",
    "sys.path.append('../scripts')\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FVI Scores\n",
    "\n",
    "Load the FVI scores from the `fvi_scores.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fvi_scores(directory):\n",
    "    fvi_scores = {}\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file == 'fvi_scores.json':\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r') as f:\n",
    "                    scores = json.load(f)\n",
    "                    fvi_scores[root] = scores\n",
    "    return fvi_scores\n",
    "\n",
    "fvi_scores = load_fvi_scores('../output/fvi_computation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a chart to show the distribution of the fvi scores\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Example fvi_scores dictionary for demonstration purposes\n",
    "fvi_scores_list = []\n",
    "for root, scores in fvi_scores.items():\n",
    "    for score in scores:\n",
    "        fvi_scores_list.append(score)\n",
    "\n",
    "df = pd.DataFrame(fvi_scores_list, columns=['fvi'])\n",
    "\n",
    "sns.histplot(df, x='fvi', bins=20)\n",
    "\n",
    "# add a bell curve\n",
    "mu, std = norm.fit(df['fvi'])\n",
    "min_fvi, max_fvi = df['fvi'].min(), df['fvi'].max()\n",
    "x = np.linspace(min_fvi, max_fvi, 100)\n",
    "y = norm.pdf(x, mu, std) * len(df) * (x[1] - x[0])\n",
    "plt.plot(x, y, 'r--', linewidth=2)\n",
    "\n",
    "# Add standard deviation lines\n",
    "plt.axvline(mu, color='blue', linestyle='--', label='Mean')\n",
    "plt.axvline(mu + std, color='green', linestyle='--', label='Mean + 1 SD')\n",
    "plt.axvline(mu - std, color='green', linestyle='--', label='Mean - 1 SD')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Frames\n",
    "\n",
    "Filter frames with FVI scores above a certain threshold.\n",
    "low FVI scores => redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 60 # optimal threshold value not neglecting important frames\n",
    "selected_frames_images = []\n",
    "selected_frames = []\n",
    "for directory, scores in fvi_scores.items():\n",
    "    for i, score in enumerate(scores):\n",
    "        if score > threshold:\n",
    "            # Adjusting the directory path to replace \"../output/ground_truth\" with \"/data\"\n",
    "            adjusted_directory = directory.replace(\"../output\", \"../data\")\n",
    "            adjusted_directory = adjusted_directory.replace(\"\\\\\", \"/\")\n",
    "            adjusted_directory_image = directory.replace(\"../output/ground_truth\", \"../output/video_frames\")\n",
    "\n",
    "            # Adjusting the directory path to replace \"/preds\" with \"\"\n",
    "            adjusted_directory_image = adjusted_directory_image.replace(\"preds\", \"\")\n",
    "\n",
    "            frame_path = os.path.join(adjusted_directory, f'{i+1:04d}.json')\n",
    "            image_path = os.path.join(adjusted_directory_image, f'{i+1:04d}.jpg')\n",
    "            image_path = image_path.replace(\"\\\\\", \"/\")\n",
    "            frame_path = frame_path.replace(\"\\\\\", \"/\")\n",
    "            selected_frames_images.append(image_path)\n",
    "            selected_frames.append(frame_path)\n",
    "\n",
    "print(len(selected_frames))\n",
    "print(selected_frames[:5])\n",
    "print(selected_frames_images[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../scripts')\n",
    "\n",
    "from coco_converter import convert_selected_frames_to_coco\n",
    "\n",
    "convert_selected_frames_to_coco(selected_frames, '../output/selected_frames_coco.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings\n",
    "\n",
    "Use a pre-trained ResNet model from PyTorch to create embeddings for the selected frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available()) # run nvidia-smi to check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from embeddings_generator import process_image, generate_embeddings\n",
    "from coco_converter import convert_selected_images_to_coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = selected_frames_images\n",
    "# Generate embeddings for all images\n",
    "embeddings = generate_embeddings(image_paths)\n",
    "valid_embeddings = [embedding for embedding in embeddings if embedding is not None]  # Filter out None values in case error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the optimal # of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "# inertia \n",
    "k_values = range(1, 100)\n",
    "inertia_values = []\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=\"auto\")\n",
    "    kmeans.fit(valid_embeddings)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    \n",
    "\n",
    "knee_locator = KneeLocator(k_values, inertia_values, curve=\"convex\", direction=\"decreasing\")\n",
    "optimal_k = knee_locator.knee\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, inertia_values, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.axvline(optimal_k, color='red', linestyle='--', label='Optimal k')\n",
    "plt.legend()\n",
    "plt.title('Elbow method for optimal number of clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the optimal number of clusters and for each cluster, calculate the centroid. Then get \"m\" many sample points per each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = optimal_k  # of clusters\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\")\n",
    "cluster_labels = kmeans.fit_predict(valid_embeddings)\n",
    "clusters = kmeans.labels_\n",
    "\n",
    "cluster_dict = []\n",
    "for idx, cluster in enumerate(cluster_labels):\n",
    "    cluster_dict.append({\"cluster\": cluster, \"embedding\": valid_embeddings[idx], \"file_path\": image_paths[idx]})\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "m = 2 # of items to sample from each cluster that is closest to the centroid\n",
    "\n",
    "# The sampled points that we want\n",
    "closest_points = {}\n",
    "\n",
    "for cluster_id, centroid in enumerate(centroids):\n",
    "    distances = np.linalg.norm(embeddings - centroid, axis=1)\n",
    "    closest_indices = np.argsort(distances)[:m]\n",
    "    closest_points[cluster_id] = closest_indices\n",
    "    \n",
    "\n",
    "# Find the closest images to each centroid\n",
    "representatives = []  # List to store representative images for each cluster\n",
    "\n",
    "# Iterate over each cluster and find the closest points\n",
    "for cluster_id, centroid in enumerate(centroids):\n",
    "    # Compute distances from all valid embeddings to the centroid\n",
    "    distances = np.linalg.norm(np.array(valid_embeddings) - centroid, axis=1)\n",
    "    \n",
    "    # Get indices of the closest images (m closest points)\n",
    "    closest_indices = np.argsort(distances)[:m]\n",
    "    \n",
    "    # Create a dictionary for this cluster with image paths and embeddings\n",
    "    cluster_representative = {\n",
    "        \"cluster\": cluster_id,\n",
    "        # \"centroid\": centroid.tolist(),  # Convert centroid to list for better readability\n",
    "        \"closest_images\": [\n",
    "            {\n",
    "                \"file_path\": image_paths[idx],\n",
    "                # \"embedding\": valid_embeddings[idx].tolist()  # Convert embedding to list\n",
    "            }\n",
    "            for idx in closest_indices\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    representatives.append(cluster_representative)\n",
    "\n",
    "\n",
    "selected_representatives_images = [] # get the actual image paths\n",
    "for rep in representatives:\n",
    "    for path in rep[\"closest_images\"]:\n",
    "        selected_representatives_images.append(path['file_path'])\n",
    "\n",
    "print(f\"Number of selected representative images: {len(selected_representatives_images)}\")\n",
    "\n",
    "# Convert list to a dictionary with keys and values being the same\n",
    "my_dict = {item: item for item in selected_representatives_images}\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open('../output/selected_representatives_image.json', 'w') as json_file:\n",
    "    json.dump(my_dict, json_file)\n",
    "\n",
    "# Convert the selected representative images to COCO format\n",
    "convert_selected_images_to_coco(selected_representatives_images, '../output/selected_representatives_coco.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Filtered Coco File and Images from Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "\n",
    "from coco_converter import categories # import coco categories for detection\n",
    "\n",
    "# Define the directory where you want to export the dataset\n",
    "export_dir = \"../output/selected_representatives_dataset\"\n",
    "\n",
    "# dataset_rest = fo.Dataset.from_images(selected_frames_images)\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    data_path=\"../output/selected_representatives_image.json\",\n",
    "    labels_path=\"../output/selected_representatives_coco.json\",\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "\n",
    "# Export the dataset to COCO format\n",
    "dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.COCODetectionDataset,\n",
    "    categories=categories\n",
    ")\n",
    "\n",
    "# Export the dataset to the specified directory\n",
    "dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.ImageDirectory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataset in Fiftyone and visualize the data + embeddings\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"resnet101\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"umap_resnet101\",\n",
    "    patches_field=\"detections\"\n",
    ")\n",
    "\n",
    "session = fo.launch_app(dataset)\n",
    "session.wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
